{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMmuW1A25Mt34DG6QUNWXgU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manavsachdevaa/NLP-learning/blob/main/Text_Preprocessing_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9c3tLZpSCvf",
        "outputId": "514e52cb-8749-417b-87da-13d7ae64e43f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text:  i am learning Python programming and Python is fast!!..\n"
          ]
        }
      ],
      "source": [
        "#Text Processing pipeline\n",
        "#Text-> Cleaning -> Tokenization -> Stopwords removal -> Stemming & lemmitization\n",
        "text=\"i am learning Python programming and Python is fast!!..\"\n",
        "print(\"Original text: \",text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step-1 Convert to lowercase\n",
        "text=text.lower()\n",
        "print(\"Lowercase:\",text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lew-Crl0SymX",
        "outputId": "28159f7c-d8f8-4bdf-c96c-5e4728154a80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lowercase: i am learning python programming and python is fast!!..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step-2: Remove Punctuation - !@#$%^&*()_;'.,\n",
        "#Using re - regex - regular expression\n",
        "import re\n",
        "text=re.sub(r'[^\\w\\s]','',text)\n",
        "#r-> Raw String (backslash)\n",
        "#Remove everything except letters and spaces\n",
        "print(\"Punctuation removed:\",text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1Ikw9lPS87n",
        "outputId": "dd41d320-2fbc-4452-cd09-18fb36b83964"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Punctuation removed: i am learning python programming and python is fast\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wnBsvjAWW00K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#step-3 Tokenization (SPlit sentence into words)\n",
        "tokens=text.split()\n",
        "print(\"Tokens: \",tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1N2w7kPuWcio",
        "outputId": "69278cdc-9916-454c-b022-f4ae4be32352"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:  ['i', 'am', 'learning', 'python', 'programming', 'and', 'python', 'is', 'fast']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step-4 Remove stop words - is,am,are,the,a,an\n",
        "stopwords=[\"a\",\"am\",\"is\",\"i\",\"the\",\"and\"]\n",
        "filtered_tokens=[]\n",
        "for word in tokens:\n",
        "  if word not in stopwords:\n",
        "    filtered_tokens.append(word)\n",
        "print(\"After removing stopwords: \",filtered_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WK8Dx4WmW5ui",
        "outputId": "7c272bc9-6c35-4ba1-c9a4-4624446d3172"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After removing stopwords:  ['learning', 'python', 'programming', 'python', 'fast']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Stemming (learning -> learn,playing-> play)  [text cleaning]\n",
        "def stem(word):\n",
        "  if word.endswith(\"ing\"):\n",
        "    return word[:-3]\n",
        "  return word\n",
        "stemmed_words=[]\n",
        "for word in filtered_tokens:\n",
        "  stemmed_words.append(stem(word))\n",
        "print(\"After stemming: \",stemmed_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKbDnIKHXxhu",
        "outputId": "aa4aea37-0e14-4be4-ea5e-bf7dd24f9d2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After stemming:  ['learn', 'python', 'programm', 'python', 'fast']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP -> phase 2-> Word to Vector"
      ],
      "metadata": {
        "id": "S6sakI7qY8a3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Why Vectors?\n",
        "#--> Because ML models only understand numbers\n",
        "\n",
        "#Bag of Words(BOW)\n",
        "#will convert -> text->count\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "oXi3gPByZMgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences=[\n",
        "    \"i am learning python and python\" ,\n",
        "    \"we are happy today\",\n",
        "    \"i am sad today\"\n",
        "]"
      ],
      "metadata": {
        "id": "1UrJeETUZifI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer=CountVectorizer()   #create object\n",
        "X=vectorizer.fit_transform(sentences)\n",
        "#learns vocabulary\n",
        "# Convert sentences to numbers"
      ],
      "metadata": {
        "id": "WGihUg62Zx8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Vocabulary :\",vectorizer.get_feature_names_out())\n",
        "#1. all unique\n",
        "#2. sorted in alphabetical order\n",
        "#3. remove single letters like i,a\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKqj0OznaV53",
        "outputId": "e2256ab6-9de3-4f27-c6b0-e3d314c2d88d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary : ['am' 'and' 'are' 'happy' 'learning' 'python' 'sad' 'today' 'we']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Vectors:\")\n",
        "print(X.toarray())\n",
        "#Gives matrix of count\n",
        "#Each row is a sentence\n",
        "#Each column is a word(vocabulary array)\n",
        "#Each value is a count\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ir6Q5J-dbv8Z",
        "outputId": "7c4c8fe9-0efd-4fa2-c2d4-7a65291e1d83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vectors:\n",
            "[[1 1 0 0 1 2 0 0 0]\n",
            " [0 0 1 1 0 0 0 1 1]\n",
            " [1 0 0 0 0 0 1 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF = Term Frequency - Inverse Document Frequency"
      ],
      "metadata": {
        "id": "3BXB_bj1coC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Why Tf-Idf?\n",
        "#Because count is not enough\n",
        "#"
      ],
      "metadata": {
        "id": "dF9nvCTKc-O5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "3X4Hq_0Hdd09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences=[\n",
        "    \"i am learning python and python\" ,\n",
        "    \"we are happy today\",\n",
        "    \"i am sad today\"\n",
        "]"
      ],
      "metadata": {
        "id": "9r4Ynq1odGAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer=TfidfVectorizer()\n",
        "X=vectorizer.fit_transform(sentences)\n",
        "#build vocabulary\n",
        "#Calculate TF\n",
        "#calculate idf - impact of each word\n",
        "#mutiplies-> tf*idf\n",
        "#produces matrix"
      ],
      "metadata": {
        "id": "az7Hnfn5dJDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Vocabulary :\",vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOyb7pqheTQC",
        "outputId": "01d24bf5-1458-487b-e2f3-405b0dd7278e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary : ['am' 'and' 'are' 'happy' 'learning' 'python' 'sad' 'today' 'we']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"TF-IDF matrix: \")\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQAqIv1zed7V",
        "outputId": "de80e96b-cec7-48e1-823e-1c709a895465"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF matrix: \n",
            "[[0.29651988 0.38988801 0.         0.         0.38988801 0.77977602\n",
            "  0.         0.         0.        ]\n",
            " [0.         0.         0.52863461 0.52863461 0.         0.\n",
            "  0.         0.40204024 0.52863461]\n",
            " [0.51785612 0.         0.         0.         0.         0.\n",
            "  0.68091856 0.51785612 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Count + Impact of each word\n",
        "#Tf - IDF rewards rare important words and penalizes common words  so that there could be no ML biasness"
      ],
      "metadata": {
        "id": "sUVOdG-he-Pc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TF=count(words)/total word\n",
        "#IDF = log(total_docs)/documents_containing_word"
      ],
      "metadata": {
        "id": "YWK1LDQSfRcN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}